<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.78.2" />


<title>Uma Possível Generalização do Método dos Mínimos Quadrados Ordinários - gexel&#39;s blog</title>
<meta property="og:title" content="Uma Possível Generalização do Método dos Mínimos Quadrados Ordinários - gexel&#39;s blog">


  <link href='https://gexel.github.io/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="https://github.com/gexel/">GitHub</a></li>
    
    <li><a href="/about/">Sobre mim</a></li>
    
    <li><a href="https://twitter.com/g_exel">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">6 min read</span>
    

    <h1 class="article-title">Uma Possível Generalização do Método dos Mínimos Quadrados Ordinários</h1>

    
    <span class="article-date">2020-11-20</span>
    

    <div class="article-content">
      
<link href="index_files/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="index_files/anchor-sections/anchor-sections.js"></script>



<div id="por-que-minimizar-a-soma-dos-quadrados-dos-resíduos" class="section level2">
<h2>Por que minimizar a soma dos <em>quadrados</em> dos resíduos?</h2>
<p>Se você acredita que uma certa variável <span class="math inline">\(\mathbf{y} = (y_1, y_2, \cdots, y_{n-1}, y_n )\)</span> é gerada por</p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i \quad , \quad \quad i = 1, 2, \cdots, n
\]</span></p>
<p>E, a partir de <span class="math inline">\(n\)</span> observações, você busca estimar <span class="math inline">\(\hat\beta_0\)</span> e <span class="math inline">\(\hat\beta_1\)</span>, como você pode calcular esses coeficientes?</p>
<p>O leitor deve estar familiarizado com o método dos <em>Mínimos Quadrados Ordinários</em> (MQO), em que se encontra os coeficientes <span class="math inline">\(\hat \beta_0\)</span> e <span class="math inline">\(\hat\beta_1\)</span> da reta de regressão <span class="math inline">\(\hat y_i = \hat \beta_0 + \hat\beta_1 x_i\)</span> minimizando a soma dos quadrados dos resíduos:</p>
<p><span class="math display">\[
\sum_{i = 1}^n \varepsilon_i^2 = \sum_{i = 1}^n (\beta_0 + \beta_1 x_i - y_i)^2
\]</span></p>
<p>Talvez você já tenha se questionado sobre o uso do “quadrado”. Por que não o valor absoluto? Por que não a décima potência? Se você já perguntou isso a um professor, talvez tenha recebido uma resposta relacionada com diferenciabilidade no ponto de mínimo<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>, mas isso é de pouca importância. Na verdade, a escolha por minimizar a soma dos <em>quadrados</em> dos resíduos não é arbitrária, e é melhor compreendida através da representação matricial do modelo:</p>
<p><span class="math display">\[
\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}
\]</span></p>
<p>Onde</p>
<p><span class="math display">\[
\mathbf{y} = \left[\begin{array}{cccc}
y_1 \\
y_2 \\
\vdots \\
y_n \\
\end{array}\right]
; \quad \quad
\mathbf{X} = \left[\begin{array}{cccc}
1 &amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1k}  \\
1 &amp; x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2k}  \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots  \\
1 &amp; x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{nk} 
\end{array}\right]
; \quad \quad
\boldsymbol{\beta} = \left[\begin{array}{cccc}
\beta_0 \\
\beta_1  \\
\vdots \\
\beta_k  \\
\end{array}\right]
\]</span></p>
<p>E, analogamente:</p>
<p><span class="math display">\[
\boldsymbol{\varepsilon} = \left[\begin{array}{cccc}
\varepsilon_1 \\
\varepsilon_2  \\
\vdots \\
\varepsilon_n  \\
\end{array}\right] = \left[\begin{array}{cccc}
\beta_0 + \beta_1 x_{11} + \beta_2 x_{12} + \cdots + \beta_k x_{1k} - y_1 \\
\beta_0 + \beta_1 x_{21} + \beta_2 x_{22} + \cdots + \beta_k x_{2k} - y_2  \\
\vdots \\
\beta_0 + \beta_1 x_{n1} + \beta_2 x_{n2} + \cdots + \beta_k x_{nk} - y_n  \\
\end{array}\right]
\]</span></p>
<p>Por esta perspectiva, o problema é encontrar <span class="math inline">\(\boldsymbol{\beta}\)</span> que minimiza <span class="math inline">\(||\boldsymbol{\varepsilon}||_2 = ||\mathbf{y} - \mathbf{X}\boldsymbol{\beta}||_2\)</span>, onde a função</p>
<p><span class="math display">\[
\begin{aligned}
||\cdot||_p \quad : \quad \mathbb{R}^n &amp;\rightarrow \mathbb{R} \\
\mathbf{x} &amp;\mapsto \bigg(\sum^n_{i=1} x_i^p \bigg)^{\frac{1}{p}} \\
\end{aligned}
\]</span></p>
<p>É a <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)#p-norm">norma <span class="math inline">\(\ell^p\)</span></a> de um vetor.</p>
<p>Para o caso da norma euclidiana (<span class="math inline">\(p = 2\)</span>), o problema fica equivalente ao problema do método dos <em>Mínimos Quadrados Ordinários</em>, uma vez que:</p>
<p><span class="math display">\[
\mathop{\mathrm{argmin}}_{\boldsymbol{\beta}} ||\boldsymbol{\varepsilon}||_2 = \mathop{\mathrm{argmin}}_{\boldsymbol{\beta}} \sum_{i = 1}^n (\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_k x_{ik} - y_i)^2
\]</span></p>
<p>Ou, no caso da regressão linear simples, em que <span class="math inline">\(k = 1\)</span>:</p>
<p><span class="math display">\[
\mathop{\mathrm{argmin}}_{\boldsymbol{\beta}}||\boldsymbol{\varepsilon}||_2 = \mathop{\mathrm{argmin}}_{(\beta_0, \beta_1)}  \sum_{i = 1}^n (\beta_0 + \beta_1 x_i - y_i)^2
\]</span></p>
<p>Se isso não é suficiente para convencer que a escolha pela minimização da soma dos <em>quadrados</em> dos resíduos é especial, note que o <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem">teorema de Gauss-Markov</a> mostra que o <em>Melhor Estimador Linear Não Viesado</em> (<em>MELNV</em>) é o estimador encontrado através do método dos mínimos quadrados ordinários. Além disso, sob a hipótese de que os erros são normalmente distribuídos, é possível mostrar que os estimadores encontrados por MQO e por <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares#Maximum_likelihood">Máxima Verossimilhança</a> são equivalentes.</p>
</div>
<div id="minimizando-a-p-ésima-potência-do-módulo-dos-resíduos" class="section level2">
<h2>Minimizando a <span class="math inline">\(p\)</span>-ésima potência do módulo dos resíduos</h2>
<p>Tendo reconhecido a notabilidade do MQO, podemos explorar a classe de estimadores encontrados pela minimização da expressão</p>
<p><span class="math display">\[
S_p(\beta_0, \beta_1) = \sum_{i = 1}^n |\beta_0 + \beta_1 x_i - y_i |^p \quad, \quad \quad p \geq 1
\]</span></p>
<p>Note o uso do <em>módulo</em>, que é importante para garantir a convexidade de casos como <span class="math inline">\(p=3\)</span>.</p>
<p>É preciso descobrir como encontrar <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> para um valor de <span class="math inline">\(p\geq1\)</span> qualquer. Infelizmente, as condições de primeira ordem não iluminam o problema, uma vez que as derivadas parciais da expressão acima não são fáceis de trabalhar:</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial}{\partial\beta_1} SR_p(\beta_0, \beta_1) &amp;= \sum_{i = 1}^n px_i(\beta_1 x_i + \beta_0 - y_i)|\beta_1 x_i + \beta_0 - y_i|^{p-2} \\
\frac{\partial}{\partial\beta_0} SR_p(\beta_0, \beta_1) &amp;= \sum_{i = 1}^n p(\beta_1 x_i + \beta_0 - y)|\beta_1 x_i + \beta_0 - y|^{p-2}
\end{align}
\]</span></p>
<p>Mais fácil que estudar as condições de primeira ordem é usar essas derivadas em um algoritmo de <em>gradient descent</em>.</p>
<p>Primeiro, defino a função <code>gradient()</code>, que pega um vetor <span class="math inline">\((\beta_1, \beta_0)\)</span> e calcula seu gradiente.</p>
<pre class="r"><code>gradient &lt;- function(x, y, p, point) {
  beta1 &lt;- point[1]
  beta0 &lt;- point[2]
  partialbeta1 &lt;- sum(p*x*(beta0 - y + x*beta1)*abs(beta0 - y + x*beta1)^(p-2))
  partialbeta0 &lt;- sum(p*(beta1*x - y + beta0)*abs(x*beta1 - y + beta0)^(p-2))
  
  return(c(partialbeta1, partialbeta0))
}</code></pre>
<p>Em seguida, é preciso definir o algoritmo iterativo. A baixa dimensão do problema nos permite usar um bastante simples: inicia-se no ponto <span class="math inline">\(\beta_1 = \beta_0 = 0\)</span> e, a cada iteração, move-se uma distância de <span class="math inline">\(0.01\)</span> unidades na direção contrária ao gradiente da função no ponto. A convexidade da função garante que não ficaremos presos em mínimos locais, pois todo mínimo local é também um mínimo global. Dessa forma, em <em>dez mil</em> passos estaremos bem perto do ponto de mínimo.</p>
<pre class="r"><code>descent &lt;- function(x, y, p) {
  point &lt;- c(0,0)
  gradi &lt;- c(10,10)
  
  for (i in 1:10000) {
    gradi &lt;- gradient(x, y, p, point)
    gamma &lt;- 0.01
    if (normv(gradi) &lt; gamma) gamma &lt;- gamma/1.5
    point &lt;- point - gamma*gradi/normv(gradi)
  }
  return(point)
}</code></pre>
<p>Em que <code>normv()</code> é uma função que calcula a norma euclidiana de um vetor.</p>
<p>Gerando alguns valores aleatórios para <span class="math inline">\(\mathbf{x}\)</span> e <span class="math inline">\(\mathbf{y}\)</span>, podemos colocar um gráfico a posição da reta de regressão a cada valor de <span class="math inline">\(p\)</span>. Abaixo, vemos gráficos onde a reta mais <span style="color:red">vermelha</span> apresenta <span style="color:red"><span class="math inline">\(p=1\)</span></span>, e a mais <span style="color:blue">azul</span> apresenta <span style="color:blue"><span class="math inline">\(p=60\)</span></span>.</p>
<p><img src="index_files/figure-html/unnamed-chunk-3-1.png" width="50%" /><img src="index_files/figure-html/unnamed-chunk-3-2.png" width="50%" /><img src="index_files/figure-html/unnamed-chunk-3-3.png" width="50%" /><img src="index_files/figure-html/unnamed-chunk-3-4.png" width="50%" /></p>
<p>Os estimadores ficam mais diferentes na presença de <em>outliers</em>, isto é, valores de <span class="math inline">\(y_i\)</span> muito distantes da média. Note que, quanto maior o valor de <span class="math inline">\(p\)</span>, maior é a tolerância por resíduos pequenos em prol de reduzir os maiores resíduos:</p>
<p><img src="index_files/figure-html/unnamed-chunk-4-1.png" width="50%" /><img src="index_files/figure-html/unnamed-chunk-4-2.png" width="50%" /><img src="index_files/figure-html/unnamed-chunk-4-3.png" width="50%" /><img src="index_files/figure-html/unnamed-chunk-4-4.png" width="50%" /></p>
<p>Em outras palavras, quanto maior <span class="math inline">\(p\)</span>, maior é o <em>peso</em> dado a resíduos grandes.</p>
<p>Animação é uma forma de visualização muito adequada para este contexto. A dimensão temporal representa a variação do valor de <span class="math inline">\(p\)</span>, que pode ser acompanhado no canto superior esquerdo de cada gráfico:</p>
<p align="center">
<p><img src="https://raw.githubusercontent.com/gexel/gexel.github.io/master/aux/reg1.gif" /></p>
<img src="https://raw.githubusercontent.com/gexel/gexel.github.io/master/aux/reg2.gif" />
</p>
<p>Note que a reta de regressão do caso <span class="math inline">\(p = 1\)</span> sempre passa por cima de pelo menos duas observações.</p>
<p>Considere uma ordenação crescente em <span class="math inline">\(y\)</span> das obervações, de forma que <span class="math inline">\((x_1, y_1)\)</span> é a observação com menor valor de <span class="math inline">\(y\)</span> e <span class="math inline">\((x_n, y_n)\)</span> é a observação com maior valor de <span class="math inline">\(y\)</span>. Enquanto que a reta de regressão do caso <span class="math inline">\(p=2\)</span> passa necessariamente por <span class="math inline">\(\bigg(\frac{\sum^n_{i=1} x_i}{n}, \frac{\sum^n_{i=1} y_i}{n}\bigg)\)</span>, a reta de regressão do caso limite <span class="math inline">\(p \rightarrow \infty\)</span> passa necessariamente por <span class="math inline">\((\frac{x_n + x_1}{2}, \frac{y_n + y_1}{2})\)</span>. A demonstração desta propriedade fica como exercício para o leitor.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Claro, se deseja-se usar as condições de primeira ordem para encontrar os estimadores analiticamente, é importante que a função seja diferenciável no seu ponto de mínimo, além de que ela precisa, antes de tudo, possuir um ponto de mínimo global. Isso descarta a função módulo e as potências ímpares. Contudo, note que a <span class="math inline">\(p\)</span>-ésima potência do módulo: <span class="math display">\[|\varepsilon_i|^p, \quad \text{com } p \geq 2\]</span> é tanto diferenciável no ponto de mínimo quanto convexa. Conclui-se que conveniência matemática não é uma justificativa apropriada para uma escolha tão consequente.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

