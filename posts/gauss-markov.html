<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2026-02-16 Mon 16:22 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Using Gauss-Markov to derive the OLS estimator.</title>
<meta name="author" content="gexel" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../static/style.css"/>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Using Gauss-Markov to derive the OLS estimator.</h1>
<p>
Consider the standard linear regression model
\[
y = X \beta + \varepsilon
\]
with fixed regressors and i.i.d. errors.
</p>

<p>
You are probably aware that the Ordinary Least Squares (OLS) estimator \(\hat \beta_{\text{OLS}}\) for \(\beta\) minimizes the Sum of Squared Residuals (RSS), i.e.
</p>
\begin{equation} \label{eq:minrss}
\hat \beta_{\text{OLS}} = \text{arg}\min_{b} \enspace (y - Xb)'(y - Xb)
\end{equation}
<p>
And, from the Gauss&#x2013;Markov theorem, you also know that among all <b>linear</b> and <b>unbiased</b> estimators of \(\beta\), the OLS estimator attains the <b>minimum variance</b>.
</p>

<p>
What a coincidence: we obtain the estimator by minimizing the RSS without constraints, and it turns out to also minimize a different objective (variance) subject to constaints (linearity and unbiasedness). How come these two problems deliver the same solution?
</p>

<p>
The standard proof of the Gauss&#x2013;Markov theorem is non-constructive. It begins with 
\(
\hat \beta_{\text{OLS}} = (X'X)^{-1}X'y
\)
and, using matrix algebra, shows that any other linear unbiased estimator has variance weakly larger than that of \(\hat \beta_{\text{OLS}}\).
</p>

<p>
Can we instead formulate a well-defined constrained minimization problem (analogous to \eqref{eq:minrss}) and solve it using standard optimization techniquesâ€”thereby deriving \(\hat \beta_{\text{OLS}}\) and proving the Gauss-Markov result simultaneously? The answer is yes, and this is what we will do after a brief digression about what &ldquo;minimum variance&rdquo; really means.
</p>

<p>
Unlike the RSS, which is always a number, \(\operatorname{Var}\left(\hat \beta_{\text{OLS}}\right)\) is a matrix. 
</p>
</div>
</body>
</html>
